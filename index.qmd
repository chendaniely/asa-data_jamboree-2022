---
title: "Data Jamboree"
subtitle: "Python"
author: "Daniel Chen"
format:
  revealjs:
    theme: simple
    footer: "[Daniel Chen](https://daniel.rbind.io/). @chendaniely. Using [Quarto](https://quarto.org/docs/presentations/revealjs/index.html). Slides: [https://github.com/chendaniely/asa-data_jamboree-2022](https://github.com/chendaniely/asa-data_jamboree-2022)"
    slide-number: c/t
    show-slide-number: all
    hash-type: number
execute:
  echo: true
jupyter: python3
---

# Exercises

## The Questions {.smaller}

1. Create a frequency table of the number of crashes by borough.
2. Create an `hour` variable with integer values from 0 to 23, and plot of the histogram of crashes by hour.
3. Check if the number of persons killed is the summation of the number of pedestrians killed, cyclist killed, and motorists killed. From now on, use the number of persons killed as the sum of the pedestrians, cyclists, and motorists killed.
4. Construct a cross table for the number of persons killed by the contributing factors of vehicle one. Collapse the contributing factors with a count of less than 100 to “other”. Is there any association between the contributing factors and the number of persons killed?
5. Create a new variable death which is one if the number of persons killed is 1 or more; and zero otherwise. Construct a cross table for death versus borough. ~~Test the null hypothesis that the two variables are not associated~~.

## The Questions {.smaller}

6. ~~Visualize the crashes using their latitude and longitude (and time, possibly in an animation).~~
7. Fit a logistic model with death as the outcome variable and covariates that are available in the data or can be engineered from the data. Example covariates are crash hour, borough, number of vehicles involved, etc. Interpret your results.
8. Aggregate the data to the zip-code level and connect with the census data at the zip-code level.
9. Visualize ~~and model~~ the count of crashes at the zip-code level.

# Question 0: The Data

## Load {.smaller}

```{python}
import pandas as pd

#jan22 = pd.read_csv("https://raw.githubusercontent.com/statds/ids-s22/main/notes/data/nyc_mv_collisions_202201.csv")
#jan22.to_csv("data/nyc_mv_collisions_202201.csv", index=False)
jan22 = pd.read_csv("data/nyc_mv_collisions_202201.csv")

jan22.head()
```

## Process {.smaller}

```{python}
import janitor

jan22 = janitor.clean_names(jan22)
jan22.head()
```

## Initial Overview

```{python}
jan22.info()
```

# Question 1

## Create a frequency table of the number of crashes by borough


```{python}
jan22["borough"].value_counts(dropna=False)
```

# Question 2

## 2a: Create an `hour` variable with integer values from 0 to 2


```{python}
jan22 = (
  jan22
  .assign(hour=jan22["crash_time"].str.split(":").str.get(0))
)
jan22[["crash_time", "hour"]].head()
```

## 2b: Plot of the histogram of crashes by hour


```{python}
from plotnine import ggplot, aes, geom_histogram, geom_bar

ggplot(jan22, aes(x="hour")) + geom_bar()
```

## It's a string!

```{python}
jan22 = jan22.assign(hour_int=pd.to_numeric(jan22["hour"]))
jan22.info()
```

## Re-plot histogram

```{python}
ggplot(jan22, aes(x="hour_int")) + geom_bar()
```


# Question 3

## Check if the number of persons killed
is the summation of the number of pedestrians killed, cyclist killed, and motorists killed.

From now on, use the number of persons killed as the sum of the pedestrians, cyclists, and motorists killed

```{python}
# check the columns we want to use
jan22.columns
```

## Subset the columns of interest {.smaller}


```{python}
casualty_columns = [
  'number_of_pedestrians_killed',
  'number_of_cyclist_killed',
  'number_of_motorist_killed',
]

killed = jan22[casualty_columns]
killed.head()
```

## Sum the columns


```{python}
# note: you do want to be careful with axis=1
killed_sum = killed.apply(sum, axis=1)
killed_sum.value_counts(dropna=False)
```


```{python}
jan22["number_of_persons_killed"].value_counts(dropna=False)
```

# Question 4

## 4a: Construct a cross table for the number of persons killed {.smaller}
by the contributing factors of vehicle one.

```{python}
cross_tab = pd.crosstab(
  jan22["contributing_factor_vehicle_1"],
  jan22["number_of_persons_killed"],
  margins=True
)

cross = cross_tab.reset_index()
cross.columns.name = None
```

## {.smaller}

```{python}
cross
```

## 4b: Collapse the contributing factors with a count of less than 100 to “other”.

```{python}
type(cross)
```

```{python}
import numpy as np

@np.vectorize
def recode_factors(factor, count):
  if count < 100:
    return "other"
  else:
    return factor
```

```{python}
recode_factors("Accelerator Defective", 9)
```

```{python}
recode_factors("Traffic Control Disregarded", 206)
```

## Create new labels

```{python}
# see the results
recode_factors(cross["contributing_factor_vehicle_1"], cross["All"])
```

## Create new labels

```{python}
# save the results
cross["recode"] = recode_factors(cross["contributing_factor_vehicle_1"], cross["All"])
cross
```

##

```{python}
recoded_cross = cross["recode"].value_counts()
recoded_cross
```

## Create the renaming dictionary

```{python}
# create the recoding dictionary
recode_dict = dict(zip(cross["contributing_factor_vehicle_1"], cross["recode"]))
recode_dict
```

## Replace/Recode the values

```{python}
jan22["cont_factor_1_recode"] = jan22.contributing_factor_vehicle_1.replace(recode_dict)

jan22[["contributing_factor_vehicle_1", "cont_factor_1_recode"]]
```


## Is there any association between the contributing factors and the number of persons killed? {.smaller}


```{python}
cross_tab_recoded = pd.crosstab(
  jan22["cont_factor_1_recode"],
  jan22["number_of_persons_killed"],
  dropna=False,
  margins=True
)
cross_tab_recoded = cross_tab_recoded.reset_index()
cross_tab_recoded.columns.name = None
```

## {.smaller}

```{python}
cross_tab_recoded
```

# Question 5

## 5a: Create a new variable death
which is one if the number of persons killed is 1 or more; and zero otherwise.

```{python}
def recode_num_killed(num_killed):
    if num_killed >= 1:
      return 1
    else:
      return 0
```

```{python}
jan22["death"] = jan22["number_of_persons_killed"].apply(recode_num_killed)
```

## {.smaller}

```{python}
jan22["death"].value_counts(dropna=False)
```


## 5b: Construct a cross table for death versus borough.

```{python}
pd.crosstab(jan22["borough"], jan22["death"])
```


<!--
## 5c: Test the null hypothesis that the two variables are not associated

```{python}
# TODO: fix model
import statsmodels.api as sm

jan22_boro_death = jan22[["borough", "death"]].dropna()
```
-->

# Question 6

## Visualize the crashes
using their latitude and longitude (and time, possibly in an animation).


```{python}
# I don't know how to plot maps in python
```

# Question 7

## Fit a logistic model
with death as the outcome variable and covariates that are available in the data or can be engineered from the data.
Example covariates are crash hour, borough, number of vehicles involved, etc. Interpret your results.


```{python}
import statsmodels.formula.api as smf

mod = smf.logit("death ~ hour_int + borough", data=jan22).fit()
```

##

```{python}
mod.summary()
```

##

```{python}
mod.params
```

## Logistic regression scikit-learn

```{python}
from sklearn import linear_model
lr = linear_model.LogisticRegression()

predictors = pd.get_dummies(
  jan22[["hour_int", "borough"]],
  drop_first=True
)

results = lr.fit(X=predictors, y = jan22["death"])
results.coef_
```

## Compare

```{python}
mod.params
```

```{python}
results.coef_
```

## Scikit-Learn's Documentation

```
class sklearn.linear_model.LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)
```

> Note that regularization is applied by default. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied).

##

```{python}
lr2 = linear_model.LogisticRegression(penalty="none")
results2 = lr2.fit(X=predictors, y = jan22["death"])
results2.coef_
```

```{python}
mod.params
```

##

```{python}
ggplot(jan22, aes(x="hour_int", y="death")) + geom_bar(stat="identity")
```

# Question 8

## 8a: Aggregate the data to the zip-code level


```{python}
jan22["zip_code_str"] = jan22["zip_code"].astype(str)
jan22["zip_code_str"]
```

##

```{python}
jan22["zip_code_str"].str.split(".").str.get(1).value_counts()
```

##

```{python}
jan22["zip_code_str"].str.len().value_counts()
```

<!--
##


```{python}
income20 = pd.read_csv("data/acs_data/ACSST5Y2020.S1903_data_with_overlays_2022-04-25T213110.csv")
```

## 8b: Connect with the census data at the zip-code level.

-->

# Question 9

## Visualize ~~and model~~ the count of crashes at the zip-code level


```{python}
jan22["zip_code_str"].value_counts()
```

##


```{python}
size_by_zip = jan22.groupby(["zip_code_str"]).size()
```


```{python}
size_by_zip_df = pd.DataFrame(size_by_zip, columns=["count"]).reset_index()
size_by_zip_df = size_by_zip_df.loc[size_by_zip_df.zip_code_str != "nan"]
```

##

```{python}
size_by_zip_df.describe()
```


##

```{python}
from plotnine import geom_bar, coord_flip
(
  ggplot(size_by_zip_df,
    aes(x="zip_code_str", y="count"))
  + geom_bar(stat="identity")
  + coord_flip()
)
```

##

```{python}
size_by_zip_df_sub = (
  size_by_zip_df
  .loc[size_by_zip_df["count"] > 50]
  .sort_values("count", ascending=False)
)
```

```{python}
size_by_zip_df_sub["zip_code_str"] = (
  size_by_zip_df_sub["zip_code_str"]
  .astype(str)
  .str.slice(start=0, stop=5)
)
```

##

```{python}
(
  ggplot(size_by_zip_df_sub,
    aes(x="zip_code_str", y="count"))
  + geom_bar(stat="identity")
  + coord_flip()
)
```
